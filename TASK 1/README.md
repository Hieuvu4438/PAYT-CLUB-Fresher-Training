# ğŸ¤– K-Nearest Neighbors (KNN) Classifier

Má»™t mini project vá» thuáº­t toÃ¡n **K-Nearest Neighbors** Ä‘Æ°á»£c implement tá»« scratch báº±ng Python.

## ğŸ“– Giá»›i thiá»‡u

K-Nearest Neighbors (KNN) lÃ  má»™t thuáº­t toÃ¡n machine learning Ä‘Æ¡n giáº£n vÃ  hiá»‡u quáº£ thuá»™c nhÃ³m **lazy learning**. Thuáº­t toÃ¡n nÃ y dá»±a trÃªn nguyÃªn lÃ½: "Nhá»¯ng Ä‘iá»ƒm gáº§n nhau thÆ°á»ng cÃ³ cÃ¹ng nhÃ£n".

### ğŸ” CÃ¡ch hoáº¡t Ä‘á»™ng:
1. **Training**: LÆ°u trá»¯ táº¥t cáº£ dá»¯ liá»‡u training
2. **Prediction**: 
   - TÃ­nh khoáº£ng cÃ¡ch tá»« Ä‘iá»ƒm cáº§n dá»± Ä‘oÃ¡n Ä‘áº¿n táº¥t cáº£ Ä‘iá»ƒm training
   - Chá»n k Ä‘iá»ƒm gáº§n nháº¥t
   - Voting Ä‘á»ƒ quyáº¿t Ä‘á»‹nh nhÃ£n (class xuáº¥t hiá»‡n nhiá»u nháº¥t)

## ğŸš€ TÃ­nh nÄƒng

### âœ¨ KNNClassifier Class
- âœ… Implementation KNN tá»« scratch
- âœ… TÃ­nh khoáº£ng cÃ¡ch Euclidean
- âœ… Thuáº­t toÃ¡n voting cho classification
- âœ… Predict probability cho tá»«ng class
- âœ… Dá»… dÃ ng tÃ¹y chá»‰nh giÃ¡ trá»‹ k

### ğŸ“Š KNNVisualizer Class
- âœ… Visualization dataset 2D
- âœ… Váº½ decision boundary
- âœ… So sÃ¡nh hiá»‡u suáº¥t vá»›i cÃ¡c giÃ¡ trá»‹ k khÃ¡c nhau
- âœ… TÃ¬m k tá»‘i Æ°u tá»± Ä‘á»™ng

### ğŸ¯ Demo Scripts
- âœ… Demo vá»›i Iris dataset (classic ML dataset)
- âœ… Demo vá»›i synthetic dataset
- âœ… ÄÃ¡nh giÃ¡ vÃ  so sÃ¡nh model

## ğŸ› ï¸ CÃ i Ä‘áº·t

### YÃªu cáº§u há»‡ thá»‘ng
- Python 3.7+
- pip

### CÃ i Ä‘áº·t dependencies
```bash
pip install -r requirements.txt
```

### Hoáº·c cÃ i Ä‘áº·t manual
```bash
pip install numpy matplotlib scikit-learn seaborn
```

## ğŸ® CÃ¡ch sá»­ dá»¥ng

### Cháº¡y demo Ä‘áº§y Ä‘á»§
```bash
python knn_classifier.py
```

### Sá»­ dá»¥ng trong code

#### 1. Basic Usage
```python
from knn_classifier import KNNClassifier

# Khá»Ÿi táº¡o model
knn = KNNClassifier(k=5)

# Training
knn.fit(X_train, y_train)

# Prediction
predictions = knn.predict(X_test)

# Probability prediction
probabilities = knn.predict_proba(X_test)
```

#### 2. Visualization
```python
from knn_classifier import KNNVisualizer

visualizer = KNNVisualizer()

# Váº½ dataset
visualizer.plot_dataset(X, y, "My Dataset")

# Váº½ decision boundary
visualizer.plot_decision_boundary(knn, X, y)

# TÃ¬m k tá»‘i Æ°u
best_k, best_accuracy = visualizer.compare_k_values(
    X_train, y_train, X_test, y_test
)
```

## ğŸ“Š Káº¿t quáº£ Demo

### ğŸŒ¸ Iris Dataset
- **Dataset**: 150 samples, 3 classes (Setosa, Versicolor, Virginica)
- **Features**: Sá»­ dá»¥ng 2 features Ä‘áº§u Ä‘á»ƒ visualization
- **Accuracy**: ~95-97% vá»›i k=5

### ğŸ”¬ Synthetic Dataset  
- **Dataset**: 300 samples, 2 classes
- **Features**: 2D synthetic data
- **Accuracy**: ~85-90% vá»›i k tá»‘i Æ°u

## ğŸ§® Thuáº­t toÃ¡n Chi tiáº¿t

### Distance Calculation
```python
def _euclidean_distance(self, point1, point2):
    return np.sqrt(np.sum((point1 - point2) ** 2))
```

### K-Neighbors Selection
```python
def _get_neighbors(self, test_point):
    distances = []
    for i, train_point in enumerate(self.X_train):
        distance = self._euclidean_distance(test_point, train_point)
        distances.append((distance, self.y_train[i]))
    
    distances.sort(key=lambda x: x[0])
    neighbors = [distances[i][1] for i in range(self.k)]
    return neighbors
```

### Voting Mechanism
```python
def predict(self, X):
    predictions = []
    for test_point in X:
        neighbors = self._get_neighbors(test_point)
        prediction = Counter(neighbors).most_common(1)[0][0]
        predictions.append(prediction)
    return np.array(predictions)
```

## ğŸ“ˆ Æ¯u Ä‘iá»ƒm & NhÆ°á»£c Ä‘iá»ƒm

### âœ… Æ¯u Ä‘iá»ƒm
- **ÄÆ¡n giáº£n**: Dá»… hiá»ƒu vÃ  implement
- **KhÃ´ng cáº§n training**: Lazy learning algorithm
- **Versatile**: CÃ³ thá»ƒ dÃ¹ng cho cáº£ classification vÃ  regression
- **Non-parametric**: KhÃ´ng giáº£ Ä‘á»‹nh vá» phÃ¢n phá»‘i dá»¯ liá»‡u

### âŒ NhÆ°á»£c Ä‘iá»ƒm
- **Cháº­m**: O(n) cho má»—i prediction
- **Memory intensive**: Pháº£i lÆ°u táº¥t cáº£ training data
- **Sensitive to noise**: Bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi outliers
- **Curse of dimensionality**: Hiá»‡u quáº£ giáº£m vá»›i nhiá»u features

## ğŸ”§ Tá»‘i Æ°u hÃ³a

### Chá»n K
- **K nhá»**: Model phá»©c táº¡p, cÃ³ thá»ƒ overfitting
- **K lá»›n**: Model Ä‘Æ¡n giáº£n, cÃ³ thá»ƒ underfitting
- **Rule of thumb**: K = âˆšn (n lÃ  sá»‘ training samples)
- **Best practice**: Sá»­ dá»¥ng cross-validation

### Distance Metrics
Hiá»‡n táº¡i sá»­ dá»¥ng Euclidean distance, cÃ³ thá»ƒ má»Ÿ rá»™ng:
- Manhattan distance
- Minkowski distance  
- Cosine similarity

## ğŸ¯ á»¨ng dá»¥ng thá»±c táº¿

1. **Recommendation Systems**: Netflix, Amazon
2. **Pattern Recognition**: Nháº­n dáº¡ng chá»¯ viáº¿t tay
3. **Computer Vision**: Classification áº£nh
4. **Text Mining**: PhÃ¢n loáº¡i vÄƒn báº£n
5. **Medical Diagnosis**: Cháº©n Ä‘oÃ¡n y táº¿

## ğŸ”® Má»Ÿ rá»™ng

### TÃ­nh nÄƒng cÃ³ thá»ƒ thÃªm:
- [ ] Weighted KNN (khoáº£ng cÃ¡ch gáº§n cÃ³ trá»ng sá»‘ cao hÆ¡n)
- [ ] KD-Tree Ä‘á»ƒ tá»‘i Æ°u tÃ¬m kiáº¿m
- [ ] Cross-validation tá»± Ä‘á»™ng
- [ ] Nhiá»u distance metrics
- [ ] KNN Regression
- [ ] Parallel processing

### Datasets khÃ¡c Ä‘á»ƒ test:
- [ ] Wine dataset
- [ ] Breast Cancer dataset
- [ ] Digits dataset
- [ ] Custom datasets

## ğŸ‘¨â€ğŸ’» TÃ¡c giáº£

**PAYT CLUB - FRESHER**  
Mini Project: K-Nearest Neighbors Implementation

## ğŸ“œ License

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c táº¡o ra cho má»¥c Ä‘Ã­ch há»c táº­p vÃ  nghiÃªn cá»©u.

---

### ğŸ‰ Happy Learning with KNN! 

*"In machine learning, the nearest neighbors are your best friends!"*
